{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from dask.distributed import Client, LocalCluster\n",
    "# import logging\n",
    "\n",
    "# cluster = LocalCluster(\n",
    "#     n_workers=28,\n",
    "#     threads_per_worker=8,\n",
    "#     silence_logs=logging.DEBUG\n",
    "# )\n",
    "\n",
    "# client = Client(cluster, heartbeat_interval=10000)\n",
    "# print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.scheduler_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.9.dev2565515056\n",
      "0.2.4.dev561125696\n"
     ]
    }
   ],
   "source": [
    "import afqinsight as afqi\n",
    "import groupyr as gpr\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "from groupyr.transform import GroupRemover, GroupExtractor\n",
    "from groupyr.decomposition import GroupPCA\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.plots import plot_convergence, plot_objective, plot_evaluations\n",
    "\n",
    "print(afqi.__version__)\n",
    "print(gpr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, groups, columns, group_names, subjects, classes = afqi.load_afq_data(\n",
    "    \"../data/raw/als_data\",\n",
    "    target_cols=[\"class\"],\n",
    "    label_encode_cols=[\"class\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_sets = afqi.multicol2sets(pd.MultiIndex.from_tuples(columns, names=[\"metric\", \"tractID\", \"nodeID\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyafq_bundles = [\n",
    "    c for c in columns\n",
    "    if c[1] not in [\"Right Cingulum Hippocampus\", \"Left Cingulum Hippocampus\"]\n",
    "]\n",
    "pyafq_bundles = [\n",
    "    [c] for c in np.unique([col[1] for col in pyafq_bundles])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = GroupRemover(\n",
    "    select=[\"Right Cingulum Hippocampus\", \"Left Cingulum Hippocampus\"],\n",
    "    groups=groups,\n",
    "    group_names=group_names,\n",
    ")\n",
    "X_pyafq_bundles = gr.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 16000)\n",
      "(48, 14400)\n",
      "16000\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X_pyafq_bundles.shape)\n",
    "print(len(label_sets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = [\n",
    "    grp for grp in group_names\n",
    "    if \"Cingulum Hippocampus\" not in grp[1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge = GroupExtractor(\n",
    "    select=[\"fa\", \"md\"],\n",
    "    groups=groups[:144],\n",
    "    group_names=group_names\n",
    ")\n",
    "X_md_fa = ge.fit_transform(X_pyafq_bundles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 16000)\n",
      "(48, 14400)\n",
      "(48, 3600)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X_pyafq_bundles.shape)\n",
    "print(X_md_fa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_md_fa = groups[:36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_cv_results(n_repeats=5, n_splits=10,\n",
    "                   power_transformer=False, \n",
    "                   shuffle=False,\n",
    "                   ensembler=None,\n",
    "                   n_estimators=10):\n",
    "    if shuffle:\n",
    "        rng = np.random.default_rng()\n",
    "        y_fit = rng.permutation(y)\n",
    "    else:\n",
    "        y_fit = np.copy(y)\n",
    "\n",
    "    X_trim = np.copy(X_md_fa)\n",
    "        \n",
    "    cv = RepeatedStratifiedKFold(\n",
    "        n_splits=n_splits,\n",
    "        n_repeats=n_repeats,\n",
    "        random_state=1729\n",
    "    )\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    gpca = GroupPCA(groups=groups_md_fa)\n",
    "    \n",
    "    cv_results = {}\n",
    "\n",
    "    for cv_idx, (train_idx, test_idx) in enumerate(cv.split(X_trim, y_fit)):\n",
    "        start = datetime.now()\n",
    "\n",
    "        X_train, X_test = X_trim[train_idx], X_trim[test_idx]\n",
    "        y_train, y_test = y_fit[train_idx], y_fit[test_idx]\n",
    "\n",
    "        groups_pca = gpca.fit(imputer.fit_transform(X_train)).groups_out_\n",
    "        \n",
    "        pipe_skopt = afqi.make_afq_classifier_pipeline(\n",
    "            imputer_kwargs={\"strategy\": \"median\"},\n",
    "            use_cv_estimator=True,\n",
    "            power_transformer=power_transformer,\n",
    "            scaler=\"standard\",\n",
    "            groups=groups_pca,\n",
    "            verbose=0,\n",
    "            pipeline_verbosity=False,\n",
    "            tuning_strategy=\"bayes\",\n",
    "            cv=3,\n",
    "            n_bayes_points=9,\n",
    "            n_jobs=28,\n",
    "            l1_ratio=[0.0, 1.0],\n",
    "            eps=5e-2,\n",
    "            n_alphas=100,\n",
    "            power_transformer_kwargs={\n",
    "                \"groups\": groups_md_fa,\n",
    "            },\n",
    "            ensemble_meta_estimator=ensembler,\n",
    "            ensemble_meta_estimator_kwargs={\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"n_jobs\": 1,\n",
    "                \"oob_score\": True,\n",
    "                \"random_state\": 1729,\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        pipe_skopt.fit(X_train, y_train)\n",
    "\n",
    "        cv_results[cv_idx] = {\n",
    "            \"pipeline\": pipe_skopt,\n",
    "            \"train_idx\": train_idx,\n",
    "            \"test_idx\": test_idx,\n",
    "            \"y_prob\": pipe_skopt.predict_proba(X_test)[:, 1],\n",
    "            \"y_pred\": pipe_skopt.predict(X_test),\n",
    "            \"y_true\": y_test,\n",
    "            \"test_accuracy\": accuracy_score(y_test, pipe_skopt.predict(X_test)),\n",
    "            \"train_accuracy\": accuracy_score(y_train, pipe_skopt.predict(X_train)),\n",
    "            \"test_roc_auc\": roc_auc_score(y_test, pipe_skopt.predict(X_test)),\n",
    "            \"train_roc_auc\": roc_auc_score(y_train, pipe_skopt.predict(X_train)),\n",
    "            \"coefs\": [\n",
    "                est.coef_ for est\n",
    "                in pipe_skopt.named_steps[\"estimate\"].estimators_\n",
    "            ],\n",
    "            \"pca_components\": pipe_skopt.named_steps[\"power_transform\"].components_,\n",
    "            \"alpha\": [\n",
    "                est.alpha_ for est\n",
    "                in pipe_skopt.named_steps[\"estimate\"].estimators_\n",
    "            ],\n",
    "            \"l1_ratio\": [\n",
    "                est.l1_ratio_ for est\n",
    "                in pipe_skopt.named_steps[\"estimate\"].estimators_\n",
    "            ],\n",
    "        }\n",
    "        \n",
    "        if ensembler is None:\n",
    "            cv_results[cv_idx][\"optimizer\"] = pipe_skopt.named_steps[\"estimate\"].bayes_optimizer_\n",
    "\n",
    "        print(f\"CV index [{cv_idx:3d}], Elapsed time: \", datetime.now() - start)\n",
    "        \n",
    "    return cv_results, y_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV index [  0], Elapsed time:  0:11:02.836631\n",
      "CV index [  1], Elapsed time:  0:12:20.902078\n",
      "CV index [  2], Elapsed time:  0:12:06.044455\n",
      "CV index [  3], Elapsed time:  0:12:14.146831\n",
      "CV index [  4], Elapsed time:  0:11:35.694972\n",
      "CV index [  5], Elapsed time:  0:11:41.146706\n",
      "CV index [  6], Elapsed time:  0:11:31.815093\n",
      "CV index [  7], Elapsed time:  0:11:26.111120\n",
      "CV index [  8], Elapsed time:  0:11:36.637716\n",
      "CV index [  9], Elapsed time:  0:11:37.123915\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "results[f\"bagging_stratify_group_pca\"] = get_cv_results(\n",
    "    n_splits=10, n_repeats=1, power_transformer=GroupPCA,\n",
    "    ensembler=\"serial-bagging\", shuffle=False,\n",
    "    n_estimators=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bagging_stratify_group_pca'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bagging_stratify_group_pca test  acc 0.8800000000000001\n",
      "bagging_stratify_group_pca train acc 0.9884249471458773\n",
      "bagging_stratify_group_pca test  auc 0.9\n",
      "bagging_stratify_group_pca train auc 0.9885281385281385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, res in results.items():\n",
    "    test_accuracies = [cvr[\"test_accuracy\"] for cvr in res[0].values()]\n",
    "    train_accuracies = [cvr[\"train_accuracy\"] for cvr in res[0].values()]\n",
    "    test_auc = [cvr[\"test_roc_auc\"] for cvr in res[0].values()]\n",
    "    train_auc = [cvr[\"train_roc_auc\"] for cvr in res[0].values()]\n",
    "    \n",
    "    print(key, \"test  acc\", np.mean(test_accuracies))\n",
    "    print(key, \"train acc\", np.mean(train_accuracies))\n",
    "    print(key, \"test  auc\", np.mean(test_auc))\n",
    "    print(key, \"train auc\", np.mean(train_auc))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"als_classify_group_pca.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def mean_over_combinations(results):\n",
    "    length = len(results)\n",
    "    mean_results = {}\n",
    "    for r in range(1, length + 1):\n",
    "        mean_results[r] = [\n",
    "            np.mean([res[\"yhat\"].values for res in comb], axis=0)\n",
    "            for comb in itertools.combinations(results, r=r)\n",
    "        ]\n",
    "        \n",
    "    return mean_results\n",
    "\n",
    "def accuracy_over_combinations(results):\n",
    "    mean_results = mean_over_combinations(results)\n",
    "    mean_accuracies = []\n",
    "    for r in mean_results.keys():\n",
    "        mean_accuracies += [\n",
    "            {\n",
    "                \"n_repeats\": r,\n",
    "                \"accuracy\": accuracy_score(results[0][\"class\"].values, res > 0.5)\n",
    "            } for res in mean_results[r]\n",
    "        ]\n",
    "        \n",
    "    return pd.DataFrame(mean_accuracies)\n",
    "\n",
    "def auc_over_combinations(results):\n",
    "    mean_results = mean_over_combinations(results)\n",
    "    mean_auc = []\n",
    "    for r in mean_results.keys():\n",
    "        mean_auc += [\n",
    "            {\n",
    "                \"n_repeats\": r,\n",
    "                \"auc\": roc_auc_score(results[0][\"class\"].values, res)\n",
    "            } for res in mean_results[r]\n",
    "        ]\n",
    "        \n",
    "    return pd.DataFrame(mean_auc)\n",
    "\n",
    "def get_accuracy_ensemble_dataframe(cv_results, y_true):    \n",
    "    test_probs = {\n",
    "        idx: pd.Series(\n",
    "            data=cvr[\"y_prob\"],\n",
    "            index=cvr[\"test_idx\"],\n",
    "            name=\"yhat\"\n",
    "        )\n",
    "        for idx, cvr in cv_results.items()\n",
    "    }\n",
    "    \n",
    "    df_ytest = {\n",
    "        idx: pd.DataFrame(test_probs[idx]).merge(\n",
    "            pd.DataFrame(y, columns=[\"class\"]),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\"\n",
    "        ) for idx in test_probs.keys()\n",
    "    }\n",
    "    \n",
    "    acc_scores = [\n",
    "        accuracy_score(_df[\"class\"].values, _df[\"yhat\"].values > 0.5)\n",
    "        for _df in df_ytest.values()\n",
    "    ]\n",
    "    \n",
    "    repeats = [\n",
    "        pd.concat([df_ytest[i] for i in range(x * 10, (x + 1) * 10)]).sort_index()\n",
    "        for x in range(len(cv_results) // 10)\n",
    "    ]\n",
    "    \n",
    "    return accuracy_over_combinations(repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_accuracies = {\n",
    "    key: get_accuracy_ensemble_dataframe(res[0], y)\n",
    "    for key, res in results.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"n_repeats\", y=\"accuracy\", data=df_accuracies[\"no_power_transform\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"n_repeats\", y=\"accuracy\", data=df_accuracies[\"shuffle_no_transform\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results[\"bagging_stratify_clone_estimators\"] = get_cv_results(\n",
    "    n_splits=10, n_repeats=1, power_transformer=False,\n",
    "    ensembler=\"serial-bagging\", shuffle=False,\n",
    "    n_estimators=20\n",
    ")\n",
    "# results[\"bagging_shuffle_stratify\"] = get_cv_results(\n",
    "#     n_splits=10, n_repeats=1, power_transformer=False,\n",
    "#     ensembler=\"serial-bagging\", shuffle=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, res in results.items():\n",
    "    test_accuracies = [cvr[\"test_accuracy\"] for cvr in res[0].values()]\n",
    "    train_accuracies = [cvr[\"train_accuracy\"] for cvr in res[0].values()]\n",
    "    print(key, \"test\", np.mean(test_accuracies))\n",
    "    print(key, \"train\", np.mean(train_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"als_classify.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_results = results[\"bagging_stratify\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_results[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_lists = [[\n",
    "    est.coef_ for est in\n",
    "    bag_res[\"pipeline\"].named_steps[\"estimate\"].estimators_\n",
    "] for bag_res in bagging_results.values()]\n",
    "\n",
    "coefs = [y for x in nested_lists for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(coefs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_md_fa = [\n",
    "    c for c in columns\n",
    "    if \"fa\" in c or \"md\" in c\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.viz import window, actor, panel\n",
    "from dipy.data import fetch_bundles_2_subjects, read_bundles_2_subjects\n",
    "from dipy.tracking.streamline import transform_streamlines\n",
    "from dipy.viz import colormap\n",
    "from dipy.viz import ui\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bag = pd.concat([\n",
    "    pd.DataFrame.from_dict(\n",
    "        {\n",
    "            k: v for k, v in bag_res.items()\n",
    "            if k in [\"test_idx\", \"y_prob\", \"y_true\"]\n",
    "        }\n",
    "    ).set_index(keys=\"test_idx\", drop=True)\n",
    "    for bag_res in bagging_results.values()\n",
    "]).sort_index()\n",
    "df_bag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bag[\"Ground truth\"] = df_bag[\"y_true\"].map({0: \"Control\", 1: \"ALS\"})\n",
    "df_bag[\"y_pred\"] = (df_bag[\"y_prob\"] > 0.5).astype(int)\n",
    "df_bag[\"Prediction\"] = df_bag[\"y_pred\"].map(\n",
    "    {0: \"Predicted control\", 1: \"Predicted ALS\"}\n",
    ")\n",
    "df_bag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "sns.swarmplot(\n",
    "    y=\"y_prob\",\n",
    "    x=\"Ground truth\",\n",
    "    hue=\"Prediction\",\n",
    "    data=df_bag,\n",
    "    ax=ax,\n",
    "    s=18\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\"Classification probabilities\", fontsize=18)\n",
    "ax.set_xlabel(\"Ground truth\", fontsize=18)\n",
    "ax.legend(fontsize=18, markerscale=2)\n",
    "ax.tick_params(axis = 'both', which = 'major', labelsize = 16)\n",
    "ax.tick_params(axis = 'both', which = 'minor', labelsize = 12)\n",
    "ax.axhline(0.5, ls=\"--\", color=\"black\")\n",
    "print(accuracy_score(df_bag[\"y_true\"], df_bag[\"y_pred\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
