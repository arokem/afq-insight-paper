{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from dask.distributed import Client, LocalCluster\n",
    "# import logging\n",
    "\n",
    "# cluster = LocalCluster(\n",
    "#     n_workers=28,\n",
    "#     threads_per_worker=8,\n",
    "#     silence_logs=logging.DEBUG\n",
    "# )\n",
    "\n",
    "# client = Client(cluster, heartbeat_interval=10000)\n",
    "\n",
    "# print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.9.dev2565515056\n"
     ]
    }
   ],
   "source": [
    "import afqinsight as afqi\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import median_absolute_error, r2_score\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error\n",
    "\n",
    "from neurocombat_sklearn import CombatModel\n",
    "from groupyr.decomposition import GroupPCA\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.plots import plot_convergence, plot_objective, plot_evaluations\n",
    "\n",
    "print(afqi.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, groups, columns, group_names, subjects, classes = afqi.load_afq_data(\n",
    "    \"../data/raw/hbn_data\",\n",
    "    target_cols=[\"Age\"],\n",
    "    index_col=\"EID\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2085, 3600)\n",
      "2085\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(len(subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2085, 1)\n"
     ]
    }
   ],
   "source": [
    "df_y = pd.read_csv(\"../data/raw/hbn_data/9994_Basic_Demos_20210329.csv\").drop(axis=\"rows\", index=0)\n",
    "df_y = df_y[[\"EID\", \"Age\"]]\n",
    "df_y = df_y.set_index(\"EID\", drop=True)\n",
    "\n",
    "df_subs = pd.DataFrame(index=subjects).merge(df_y, how=\"left\", left_index=True, right_index=True)\n",
    "df_subs = df_subs[~df_subs.index.duplicated(keep=\"first\")]\n",
    "print(df_subs.shape)\n",
    "\n",
    "y = df_subs[\"Age\"].astype(np.float64).to_numpy()\n",
    "nan_mask = np.logical_not(np.isnan(y))\n",
    "y = y[nan_mask]\n",
    "X = X[nan_mask, :]\n",
    "subjects = list(np.array(subjects)[nan_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999, 3600)\n",
      "(1999,)\n",
      "1999\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(len(subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1651, 3600)\n",
      "(1651,)\n",
      "1651\n",
      "(1651,)\n",
      "sessionID    0\n",
      "site_idx     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_sites = pd.read_csv(\n",
    "    \"../data/raw/hbn_data/combined_tract_profiles-20210128.csv\", usecols=[\"subjectID\", \"sessionID\"]\n",
    ").drop_duplicates().set_index(\"subjectID\")\n",
    "\n",
    "df_sites = pd.DataFrame(index=subjects).merge(\n",
    "    df_sites, how=\"left\", left_index=True, right_index=True\n",
    ")\n",
    "\n",
    "site_mask = np.logical_and(\n",
    "    df_sites[\"sessionID\"] != \"HBNsiteSI\",\n",
    "    df_sites[\"sessionID\"] != \"HBNsiteCUNY\"\n",
    ").to_numpy()\n",
    "\n",
    "df_sites = df_sites[site_mask]\n",
    "X = X[site_mask]\n",
    "y = y[site_mask]\n",
    "subjects = list(np.array(subjects)[site_mask])\n",
    "\n",
    "df_sites[\"site_idx\"] = df_sites[\"sessionID\"].map({\"HBNsiteRU\": 0, \"HBNsiteCBIC\": 1})\n",
    "sites = df_sites[\"site_idx\"].to_numpy()\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(len(subjects))\n",
    "print(sites.shape)\n",
    "print(df_sites.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_cv_results(n_repeats=5, n_splits=10,\n",
    "                   power_transformer=False, \n",
    "                   shuffle=False,\n",
    "                   ensembler=None,\n",
    "                   target_transform_func=None,\n",
    "                   target_transform_inverse_func=None,\n",
    "                   n_estimators=10):\n",
    "    if shuffle:\n",
    "        rng = np.random.default_rng()\n",
    "        y_fit = rng.permutation(y)\n",
    "    else:\n",
    "        y_fit = np.copy(y)\n",
    "    \n",
    "    X_trim = np.copy(X)\n",
    "    \n",
    "    cv = RepeatedKFold(\n",
    "        n_splits=n_splits,\n",
    "        n_repeats=n_repeats,\n",
    "        random_state=1729\n",
    "    )\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    gpca = GroupPCA(groups=groups)\n",
    "\n",
    "    cv_results = {}\n",
    "\n",
    "    for cv_idx, (train_idx, test_idx) in enumerate(cv.split(X_trim, y_fit)):\n",
    "        start = datetime.now()\n",
    "\n",
    "        X_train, X_test = X_trim[train_idx], X_trim[test_idx]\n",
    "        y_train, y_test = y_fit[train_idx], y_fit[test_idx]\n",
    "        sites_train, sites_test = sites[train_idx].reshape(-1, 1), sites[test_idx].reshape(-1, 1)\n",
    "\n",
    "        imputer = SimpleImputer(strategy=\"median\")\n",
    "        X_train_imputed = imputer.fit_transform(X_train)\n",
    "        X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "        combat = CombatModel()\n",
    "        X_train = combat.fit_transform(X_train_imputed, sites_train)\n",
    "        X_test = combat.transform(X_test_imputed, sites_test)\n",
    "\n",
    "        groups_pca = gpca.fit(X_train).groups_out_\n",
    "\n",
    "        pipe_skopt = afqi.make_afq_regressor_pipeline(\n",
    "            imputer_kwargs={\"strategy\": \"median\"},\n",
    "            use_cv_estimator=True,\n",
    "            power_transformer=power_transformer,\n",
    "            scaler=\"standard\",\n",
    "            groups=groups_pca,\n",
    "            verbose=0,\n",
    "            pipeline_verbosity=False,\n",
    "            tuning_strategy=\"bayes\",\n",
    "            cv=3,\n",
    "            n_bayes_points=9,\n",
    "            n_jobs=28,\n",
    "            l1_ratio=[0.0, 1.0],\n",
    "            eps=5e-2,\n",
    "            n_alphas=100,\n",
    "            power_transformer_kwargs={\n",
    "                \"groups\": groups\n",
    "            },\n",
    "            ensemble_meta_estimator=ensembler,\n",
    "            ensemble_meta_estimator_kwargs={\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"n_jobs\": 1,\n",
    "                \"oob_score\": True,\n",
    "                \"random_state\": 1729,\n",
    "            },\n",
    "            target_transform_func=target_transform_func,\n",
    "            target_transform_inverse_func=target_transform_inverse_func,\n",
    "        )\n",
    "\n",
    "        pipe_skopt.fit(X_train, y_train)\n",
    "\n",
    "        cv_results[cv_idx] = {\n",
    "            \"pipeline\": pipe_skopt,\n",
    "            \"train_idx\": train_idx,\n",
    "            \"test_idx\": test_idx,\n",
    "            \"y_pred\": pipe_skopt.predict(X_test),\n",
    "            \"y_true\": y_test,\n",
    "            \"test_mae\": median_absolute_error(y_test, pipe_skopt.predict(X_test)),\n",
    "            \"train_mae\": median_absolute_error(y_train, pipe_skopt.predict(X_train)),\n",
    "            \"test_r2\": r2_score(y_test, pipe_skopt.predict(X_test)),\n",
    "            \"train_r2\": r2_score(y_train, pipe_skopt.predict(X_train)),\n",
    "            \"pca_components\": pipe_skopt.named_steps[\"power_transform\"].components_,\n",
    "        }\n",
    "        \n",
    "        if ((target_transform_func is not None)\n",
    "            or (target_transform_inverse_func is not None)):\n",
    "            cv_results[cv_idx][\"coefs\"] = [\n",
    "                est.coef_ for est\n",
    "                in pipe_skopt.named_steps[\"estimate\"].regressor_.estimators_\n",
    "            ]\n",
    "            cv_results[cv_idx][\"alpha\"] = [\n",
    "                est.alpha_ for est\n",
    "                in pipe_skopt.named_steps[\"estimate\"].regressor_.estimators_\n",
    "            ]\n",
    "            cv_results[cv_idx][\"l1_ratio\"] = [\n",
    "                est.l1_ratio_ for est\n",
    "                in pipe_skopt.named_steps[\"estimate\"].regressor_.estimators_\n",
    "            ]\n",
    "        else:\n",
    "            cv_results[cv_idx][\"coefs\"] = [\n",
    "                est.coef_ for est\n",
    "                in pipe_skopt.named_steps[\"estimate\"].estimators_\n",
    "            ]\n",
    "            cv_results[cv_idx][\"alpha\"] = [\n",
    "                est.alpha_ for est\n",
    "                in pipe_skopt.named_steps[\"estimate\"].estimators_\n",
    "            ]\n",
    "            cv_results[cv_idx][\"l1_ratio\"] = [\n",
    "                est.l1_ratio_ for est\n",
    "                in pipe_skopt.named_steps[\"estimate\"].estimators_\n",
    "            ]\n",
    "\n",
    "        if ensembler is None:\n",
    "            if ((target_transform_func is not None)\n",
    "                or (target_transform_inverse_func is not None)):\n",
    "                cv_results[cv_idx][\"optimizer\"] = pipe_skopt.named_steps[\"estimate\"].regressor_.bayes_optimizer_                \n",
    "            else:\n",
    "                cv_results[cv_idx][\"optimizer\"] = pipe_skopt.named_steps[\"estimate\"].bayes_optimizer_\n",
    "\n",
    "        print(f\"CV index [{cv_idx:3d}], Elapsed time: \", datetime.now() - start)\n",
    "        \n",
    "    return cv_results, y_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV index [  0], Elapsed time:  0:39:39.825363\n",
      "CV index [  1], Elapsed time:  0:40:25.377193\n",
      "CV index [  2], Elapsed time:  0:37:07.703162\n",
      "CV index [  3], Elapsed time:  0:23:51.643549\n",
      "CV index [  4], Elapsed time:  0:24:28.524248\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "results[f\"bagging_target_transform_group_pca\"] = get_cv_results(\n",
    "    n_splits=5, n_repeats=1, power_transformer=GroupPCA,\n",
    "    ensembler=\"serial-bagging\", shuffle=False, n_estimators=20,\n",
    "    target_transform_func=np.log, target_transform_inverse_func=np.exp,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bagging_target_transform_group_pca mean MAE 1.6078063688106763\n",
      "bagging_target_transform_group_pca std MAE 0.08270609605467777\n",
      "bagging_target_transform_group_pca mean R2 0.4100130061512738\n",
      "bagging_target_transform_group_pca std R2 0.035349582128947\n"
     ]
    }
   ],
   "source": [
    "for key, res in results.items():\n",
    "    test_mae = [cvr[\"test_mae\"] for cvr in res[0].values()]\n",
    "    train_mae = [cvr[\"train_mae\"] for cvr in res[0].values()]\n",
    "    test_r2 = [cvr[\"test_r2\"] for cvr in res[0].values()]\n",
    "    train_r2 = [cvr[\"train_r2\"] for cvr in res[0].values()]\n",
    "\n",
    "    print(key, \"mean MAE\", np.mean(test_mae))\n",
    "    print(key, \"std MAE\", np.std(test_mae))\n",
    "    print(key, \"mean R2\", np.mean(test_r2))\n",
    "    print(key, \"std R2\", np.std(test_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hbn_regression_group_pca.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
